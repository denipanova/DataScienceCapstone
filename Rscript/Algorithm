The R file creates ngram frequency table. 
The data that is used consists only of the blog and the news articles. The twitter messages are removed because the text has a lot of spelling mistakes which would skew the predictions.
The algorithm: 
* Load the two data sets
* Sentence all the articles because we want to predict next word based on the sentence completion. For example, if we have the two sentences:
"I am blond. I am tall." If we first remove all fullstops and then create ngrams, we will have additional ngram - 'blond I'. But we would like to have gramatically full ngrams, which would be:
'I am', 'am blond', 'am tall'
* Clean the text 
* Create ngrams 
* Save the first component and the second component in two seperate columns in order to search more easy for the next word
* Order the data by frequency of the ngrams because it will be faster to search through the data in that manner
* Remove all entries which have been observed only once in order to reduce the data, because it take a lot for the shiny app to load it

